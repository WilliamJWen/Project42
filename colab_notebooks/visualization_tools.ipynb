{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTgDSEhTgWlY9lOoUCHuIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamJWen/Project42/blob/main/colab_notebooks/visualization_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "eQB9n6wEA8xJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your Model with Dummy Input\n"
      ],
      "metadata": {
        "id": "HD3gNU5TCXnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "# Copy Your Model code here #\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Baseline, self).__init__()\n",
        "\n",
        "    # Hidden layer activation\n",
        "    self.activation = F.relu\n",
        "\n",
        "    # Pooling\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 2w x 2h -> w x h\n",
        "    self.avgpool4 = nn.AvgPool2d(kernel_size=4, stride=4) # 4w x 4h -> w x h\n",
        "\n",
        "    # Convolutional layers\n",
        "    self.conv1 = nn.Conv2d(in_channels=3,\n",
        "                           out_channels=64,\n",
        "                           kernel_size=7,\n",
        "                           padding=3,\n",
        "                           stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=64,\n",
        "                           out_channels=64,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=1)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64,\n",
        "                           out_channels=64,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=1)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64,\n",
        "                           out_channels=128,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=2)\n",
        "    self.conv5 = nn.Conv2d(in_channels=128,\n",
        "                           out_channels=128,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=1)\n",
        "    self.conv6 = nn.Conv2d(in_channels=128,\n",
        "                           out_channels=256,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=2)\n",
        "    self.conv7 = nn.Conv2d(in_channels=256,\n",
        "                           out_channels=256,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=1)\n",
        "\n",
        "    # FC layer\n",
        "    self.fc = nn.Linear(256 * 4 * 4, 1)\n",
        "\n",
        "    # Batch normalization\n",
        "    self.norm64 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm128 = nn.BatchNorm2d(num_features=128)\n",
        "    self.norm256 = nn.BatchNorm2d(num_features=256)\n",
        "\n",
        "\n",
        "  def downsample2(self, x, out_channels):\n",
        "    N, C, H, W = x.shape # Assume H == W\n",
        "\n",
        "    # Downsample by 2\n",
        "    downsample = nn.AvgPool2d(2, 2)\n",
        "    x = downsample(x)\n",
        "\n",
        "    # 0 padding new channels\n",
        "    new_channels = out_channels - C\n",
        "    x = F.pad(x, pad=(0, 0, 0, 0, 0, new_channels))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Layer 1\n",
        "    x = self.conv1(x)                                       # Output: 64x128x128\n",
        "    x = self.norm64(self.activation(self.maxpool2(x)))      # Output: 64x64x64\n",
        "\n",
        "    # Layer 2\n",
        "    skip = x.detach().clone()\n",
        "    x = self.norm64(self.activation(self.conv2(x)))         # Output: 64x64x64\n",
        "\n",
        "    # Layer 3\n",
        "    x = self.norm64(self.activation(skip + self.conv3(x)))  # Output: 64x64x64\n",
        "\n",
        "    # Layer 4\n",
        "    skip = x.detach().clone()\n",
        "    x = self.norm128(self.conv4(x))                         # Output: 128x32x32\n",
        "\n",
        "    # Layer 5\n",
        "    x = self.norm128(self.activation(self.downsample2(skip, 128)\n",
        "                                      + self.conv5(x)))     # Output: 128x32x32\n",
        "\n",
        "    # Layer 6\n",
        "    skip = x.detach().clone()\n",
        "    x = self.norm256(self.activation(self.conv6(x)))        # Output: 256x16x16\n",
        "\n",
        "    # Layer 7\n",
        "    x = self.downsample2(skip, 256) + self.conv7(x)         # Output: 256x16x16\n",
        "    x = self.norm256(self.activation(self.avgpool4(x)))     # Output: 256x4x4\n",
        "\n",
        "    # Layer 8\n",
        "    x = x.view(-1, 256 * 4 * 4)\n",
        "    x = self.fc(x)\n",
        "    x = x.squeeze(1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = Baseline()\n",
        "num_params = 0\n",
        "for param in model.parameters():\n",
        "    num_params += param.numel()\n",
        "print(\"There are\", num_params, \"parameters in the baseline model\")\n",
        "\n",
        "#############################################\n",
        "# Customize your dummy_input\n",
        "dummy_input = torch.randn(1, 3, 256, 256)"
      ],
      "metadata": {
        "id": "OnnEU6zUCbt-",
        "outputId": "e885a8c4-f653-43c3-a793-f96abbed2c3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1195009 parameters in the baseline model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Netron"
      ],
      "metadata": {
        "id": "gCCgBJjyCBjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netron"
      ],
      "metadata": {
        "id": "GPIeAaMECIGu",
        "outputId": "de1d2484-eb89-4bbc-fdeb-8e0d146f05cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netron\n",
            "  Downloading netron-8.1.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading netron-8.1.9-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: netron\n",
            "Successfully installed netron-8.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.onnx\n",
        "import netron"
      ],
      "metadata": {
        "id": "Y19X9co4CGqL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n"
      ],
      "metadata": {
        "id": "eJ6tbu6iEA50",
        "outputId": "89a650ac-027b-4274-9d5d-d2d11afc3c87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.onnx.export(model, dummy_input, \"model.onnx\")\n",
        "\n",
        "# Open the .onnx file in https://netron.app/"
      ],
      "metadata": {
        "id": "38TsuiabCSRm",
        "outputId": "3547c8ce-2211-4b29-9874-9ff8ea606057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serving 'model.onnx' at http://localhost:8081\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('localhost', 8081)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8L-12Ma8EkWO",
        "outputId": "b829e186-689b-4962-9a4f-85913f9609fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serving '/content/model.onnx' at http://localhost:24345\n",
            "\n",
            "Stopping http://localhost:24345\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/netron/server.py\", line 265, in wait\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/netron\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/netron/__init__.py\", line 37, in main\n",
            "    wait()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/netron/server.py\", line 268, in wait\n",
            "    stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/netron/server.py\", line 250, in stop\n",
            "    thread.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/netron/server.py\", line 158, in stop\n",
            "    self.stop_event.set()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 599, in set\n",
            "    self._cond.notify_all()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 403, in notify_all\n",
            "    self.notify(len(self._waiters))\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 376, in notify\n",
            "    if not self._is_owned():\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 286, in _is_owned\n",
            "    def _is_owned(self):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torchviz"
      ],
      "metadata": {
        "id": "ur751ucsB-jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz\n",
        "from torchviz import make_dot"
      ],
      "metadata": {
        "id": "5KgWIUXWBQKt",
        "outputId": "a94d061f-71db-44b4-d41c-0e93acba1006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.5.1+cu124)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torchviz)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torchviz)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torchviz)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torchviz)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torchviz)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torchviz)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torchviz)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchviz\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchviz-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "# Copy Your Model code here #\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Baseline, self).__init__()\n",
        "\n",
        "    # Hidden layer activation\n",
        "    self.activation = F.relu\n",
        "\n",
        "    # Pooling\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 2w x 2h -> w x h\n",
        "    self.avgpool4 = nn.AvgPool2d(kernel_size=4, stride=4) # 4w x 4h -> w x h\n",
        "\n",
        "    # Convolutional layers\n",
        "    self.conv1 = nn.Conv2d(in_channels=3,\n",
        "                           out_channels=64,\n",
        "                           kernel_size=7,\n",
        "                           padding=3,\n",
        "                           stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=64,\n",
        "                           out_channels=64,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=1)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64,\n",
        "                           out_channels=64,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=1)\n",
        "    self.conv4 = nn.Conv2d(in_channels=64,\n",
        "                           out_channels=128,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=2)\n",
        "    self.conv5 = nn.Conv2d(in_channels=128,\n",
        "                           out_channels=128,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=1)\n",
        "    self.conv6 = nn.Conv2d(in_channels=128,\n",
        "                           out_channels=256,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=2)\n",
        "    self.conv7 = nn.Conv2d(in_channels=256,\n",
        "                           out_channels=256,\n",
        "                           kernel_size=3,\n",
        "                           padding=1,\n",
        "                           stride=1)\n",
        "\n",
        "    # FC layer\n",
        "    self.fc = nn.Linear(256 * 4 * 4, 1)\n",
        "\n",
        "    # Batch normalization\n",
        "    self.norm64 = nn.BatchNorm2d(num_features=64)\n",
        "    self.norm128 = nn.BatchNorm2d(num_features=128)\n",
        "    self.norm256 = nn.BatchNorm2d(num_features=256)\n",
        "\n",
        "\n",
        "  def downsample2(self, x, out_channels):\n",
        "    N, C, H, W = x.shape # Assume H == W\n",
        "\n",
        "    # Downsample by 2\n",
        "    downsample = nn.AvgPool2d(2, 2)\n",
        "    x = downsample(x)\n",
        "\n",
        "    # 0 padding new channels\n",
        "    new_channels = out_channels - C\n",
        "    x = F.pad(x, pad=(0, 0, 0, 0, 0, new_channels))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Layer 1\n",
        "    x = self.conv1(x)                                       # Output: 64x128x128\n",
        "    x = self.norm64(self.activation(self.maxpool2(x)))      # Output: 64x64x64\n",
        "\n",
        "    # Layer 2\n",
        "    skip = x.detach().clone()\n",
        "    x = self.norm64(self.activation(self.conv2(x)))         # Output: 64x64x64\n",
        "\n",
        "    # Layer 3\n",
        "    x = self.norm64(self.activation(skip + self.conv3(x)))  # Output: 64x64x64\n",
        "\n",
        "    # Layer 4\n",
        "    skip = x.detach().clone()\n",
        "    x = self.norm128(self.conv4(x))                         # Output: 128x32x32\n",
        "\n",
        "    # Layer 5\n",
        "    x = self.norm128(self.activation(self.downsample2(skip, 128)\n",
        "                                      + self.conv5(x)))     # Output: 128x32x32\n",
        "\n",
        "    # Layer 6\n",
        "    skip = x.detach().clone()\n",
        "    x = self.norm256(self.activation(self.conv6(x)))        # Output: 256x16x16\n",
        "\n",
        "    # Layer 7\n",
        "    x = self.downsample2(skip, 256) + self.conv7(x)         # Output: 256x16x16\n",
        "    x = self.norm256(self.activation(self.avgpool4(x)))     # Output: 256x4x4\n",
        "\n",
        "    # Layer 8\n",
        "    x = x.view(-1, 256 * 4 * 4)\n",
        "    x = self.fc(x)\n",
        "    x = x.squeeze(1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "baseline_model = Baseline()\n",
        "num_params = 0\n",
        "for param in baseline_model.parameters():\n",
        "    num_params += param.numel()\n",
        "print(\"There are\", num_params, \"parameters in the baseline model\")\n"
      ],
      "metadata": {
        "id": "bD5J6obUBEGT",
        "outputId": "13fdbf19-cfd7-4ac1-fe4d-06a7ee0e5da0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1195009 parameters in the baseline model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Baseline()\n",
        "dummy_input = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "output = model(dummy_input)\n",
        "make_dot(output, params=dict(model.named_parameters())).render(\"model_graph\", format=\"png\")"
      ],
      "metadata": {
        "id": "SJZTL1S0A_v6",
        "outputId": "95673bcd-ef0d-41c2-e260-c366bcd38a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_graph.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph"
      ],
      "metadata": {
        "id": "PLiDzx62HHzD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dot = Digraph()\n",
        "\n",
        "dot.node(\"Input\", \"28x28x1\")\n",
        "dot.node(\"Conv1\", \"Conv (5x5)\")\n",
        "dot.node(\"Pool1\", \"Max-Pool (2x2)\")\n",
        "dot.node(\"Conv2\", \"Conv (5x5)\")\n",
        "dot.node(\"Pool2\", \"Max-Pool (2x2)\")\n",
        "dot.node(\"Flatten\", \"Flatten\")\n",
        "dot.node(\"FC1\", \"Fully-Connected\")\n",
        "dot.node(\"FC2\", \"Fully-Connected (Output)\")\n",
        "\n",
        "# Correctly define edges as a list of (tail, head) tuples\n",
        "dot.edges([(\"Input\", \"Conv1\"),  # Edge from Input to Conv1\n",
        "           (\"Conv1\", \"Pool1\"),  # Edge from Conv1 to Pool1\n",
        "           (\"Pool1\", \"Conv2\"),  # Edge from Pool1 to Conv2\n",
        "           (\"Conv2\", \"Pool2\"),  # Edge from Conv2 to Pool2\n",
        "           (\"Pool2\", \"Flatten\"),# Edge from Pool2 to Flatten\n",
        "           (\"Flatten\", \"FC1\"),  # Edge from Flatten to FC1\n",
        "           (\"FC1\", \"FC2\")])    # Edge from FC1 to FC2\n",
        "\n",
        "\n",
        "dot.render(\"cnn_architecture\", format=\"png\", cleanup=True)\n"
      ],
      "metadata": {
        "id": "q_2arJw9HHNR",
        "outputId": "2bc305a4-bf61-4df8-cae7-bf9566799aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cnn_architecture.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}